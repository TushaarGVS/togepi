general:
  device: 'auto'

transformer:
  id: 'togepi'
  args:
    embedding_dim: 768
    num_token_types: 2
    hidden_dim: 768
    num_enc_layers: 12
    num_attn_heads: 12
    max_position_embeddings: 513
    use_togepi_mha: True
    softmax_psf_weights: True
    attn_actn: 'gelu'
    use_spectral_norm: True
    sparse_init_dens: null
    num_power_iters: 1
    attn_dropout_proba: 0.1
    embedding_dropout_proba: 0.1
    keep_prob: 1.0
    causal_attn: True
    use_explicit_lm_head: False
    pad_position: 0
    pad_token_type: 0

optim:
  id: 'adam'
  args:
    lr: 2.0e-3
    betas: [ 0.9, 0.999 ]
    eps: 1.0e-9

trainer:
  id: 'trainer'
  args:
    sparse_dens: 0.3
    sparse_penalty_lambda: 0.05
    labels_ignore_idx: 0
    gradient_clip_value: 1.0
    num_workers: 4
    noam_num_warmup_steps: 4000
    noam_factor: 1.0
    rop_patience: 5
    rop_factor: 0.5

train_and_eval:
  args:
    batch_size: 4
    grad_update_every: 4
    num_steps_per_epoch: null
    num_epochs: 20
    checkpoint_every: 1

test:
  args:
    batch_size: 4
    labels_ignore_idx: 0
    num_workers: 4

generate:
  args:
    max_new_tokens: 100
    temperature: 0.85
    num_samples: 10
    top_k: 50

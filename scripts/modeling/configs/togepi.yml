general:
  device: 'auto'

transformer:
  id: 'togepi'
  args:
    embedding_dim: 786
    num_token_types: 2
    hidden_dim: 1024
    num_enc_layers: 12
    num_attn_heads: 12
    max_position_embeddings: 1025
    use_togepi_mha: True
    softmax_psf_weights: True
    sparse_dens: 0.3
    attn_actn: 'gelu'
    use_spectral_norm: True
    sparse_init_dens: null
    num_power_iters: 1
    attn_dropout_proba: 0.1
    embedding_dropout_proba: 0.1
    keep_prob: 1.0
    causal_attn: True
    use_explicit_lm_head: False
    pad_position: 0
    pad_token_type: 0

optim:
  id: 'adam'
  args:
    lr: 2.0e-3
    betas: [ 0.9, 0.999 ]
    eps: 1.0e-9

trainer:
  id: 'togepi_trainer'
  args:
    sparse_dens: 0.3
    sparse_penalty_lambda: 0.05
    labels_ignore_idx: 0
    gradient_clip_value: 0.75
    use_amp: True
    num_workers: 4
    noam_num_warmup_steps: 4000
    noam_factor: 1.0
    rop_patience: 5
    rop_factor: 0.1

train_and_eval:
  args:
    batch_size: 32
    num_steps_per_epoch: null
    num_epochs: 20
    checkpoint_every: 1

generate:
  args:
    max_new_tokens: 100
    temperature: 1.0
    num_samples: 10
    top_k: 50

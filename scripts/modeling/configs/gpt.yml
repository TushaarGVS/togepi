general:
  device: 'auto'

transformer:
  id: 'gpt'
  args:
    embedding_dim: 768
    num_token_types: 2
    hidden_dim: 1024
    num_enc_layers: 12
    num_attn_heads: 12
    max_position_embeddings: 513
    use_togepi_mha: False
    attn_dropout_proba: 0.1
    embedding_dropout_proba: 0.1
    keep_prob: 1.0
    causal_attn: True
    use_explicit_lm_head: False
    pad_position: 0
    pad_token_type: 0

optim:
  id: 'adamw'
  args:
    lr: 2.0e-4
    betas: [ 0.9, 0.98 ]
    eps: 1.0e-6
    weight_decay: 0.01

trainer:
  id: 'trainer'
  args:
    labels_ignore_idx: 0
    gradient_clip_norm: 1.0
    num_workers: 4
    use_amp: True
    use_dp: True
    noam_num_warmup_steps: 4000
    noam_factor: 1.0
    rop_patience: 5
    rop_factor: 0.5

train_and_eval:
  args:
    batch_size: 32
    grad_update_every: 32
    num_steps_per_epoch: null
    num_epochs: 20
    checkpoint_every: 1

test:
  args:
    batch_size: 128
    labels_ignore_idx: 0
    num_workers: 4
    use_amp: True

generate:
  args:
    max_new_tokens: 100
    temperature: 0.85
    num_samples: 10
    top_k: 50

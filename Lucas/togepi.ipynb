{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 5223 - Grad Project\n",
    "Tushaar Gangavarapu - TG352 and Lucas Molter - LM865\n",
    "\n",
    "Professor: David Bindel\n",
    "\n",
    "May 19th 2023\n",
    "\n",
    "Cornell University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "The backbone of recent (and important) NLP developments has been the Attention matrix, first mentioned at the 2017 paper \"Attention is All You Need\". To explain it very briefly, this approach has allowed NLP models represent words as a weighted average of other words.\n",
    "\n",
    "This idea might seem odd at first, but if we think about it we can understand the underlying motivation. In a sentence such as:\n",
    "\n",
    "*1. The __ball__ broke the window when the kid kicked it.*\n",
    "\n",
    "*2. The Universities's winter __ball__ was very well organized.*\n",
    "\n",
    "\n",
    "It is only possible to understand the meaning of __ball__ in each sentence given the context, which is equivalent to saying that the true meaning of __ball__ is conditioned to the other words in the sentence. For us, humans, this process is naturally learned when we learn how to speak, but for NLP models the solution found was the computation of the Attention matrix:\n",
    "\n",
    "$$Attention = softmax\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\begin{align}\n",
    "Q &= XW^Q \\in \\mathbb{R}^{L \\times d}\\\\\n",
    "K &= XW^K \\in \\mathbb{R}^{L \\times d}\\\\\n",
    "V &= XW^V \\in \\mathbb{R}^{L \\times d}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "And more explicitly:\n",
    "\n",
    "$$\\begin{align}\n",
    "X \\in \\mathbb{R}^{L \\times d}&= input \\ embedding \\equiv tokenized \\ and \\ projected \\ words\\\\\n",
    "L &= input \\ sentence \\ maximum \\ length\\\\\n",
    "d &= embedding \\ dimension\\\\\n",
    "W^{Q} \\in \\mathbb{R}^{d \\times d} &= Model \\ projection \\ matrix \\ Q\\\\\n",
    "W^{K} \\in \\mathbb{R}^{d \\times d} &= Model \\ projection \\ matrix \\ K\\\\\n",
    "W^{V} \\in \\mathbb{R}^{d \\times d} &= Model \\ projection \\ matrix \\ V\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "There is a lot of NLP interpretations and nuances that we could explore here, but there would just make this report long (and possible boring). The main aspect here and theme of our CS5223 Grad project is the final structure that this Attention matrix assumes. As we can see in the images bellow, it is basically a diagonal matrix (convolution across neighbours or cells within a certain radius) added with a sparse matrix that takes care of the long range dependencies (just a pinch of NLP intuition here).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1IRjcD8v6xCp_8iL1DaAl5ZPejb3gnN0N\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image bellow is just a toy ilustration, but indeed helps to visualize the diagonal pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1E7yOoPT4hChNjIXX22V6IdAeObEZ7RHe\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, as discussed multiples times before, inspired by our project two, we decided to use FFT to compute this convolution faster and create a sparse light and easy to deal structure to accoun for the long range dependencies. Our expectation was a significant improvement in terms of speed given the simplificatin of the operations. \n",
    "\n",
    "Before deriving our final approach, there is one final detail that is important to mention. Usually, the Attention operations happens in what is called \"Multi-head Attention\". Basically it means that the dimension $d$ is going to be broken up into $n$ (considering $n$ heads) pieces of $\\frac{d}{n}$. Each one of those pieces is individually processed by its head attention. In the end of the process all the outputs (from each head) are combined and reprojected to the same space by the matrix $V$. For more details and better intuition we recommend reading the paper \"Attention is All You Need\". However, to simplify and enhence comprehension, let's consider a single head, at least for now.\n",
    "\n",
    "In more mathematical terms, the attention is as follows:\n",
    "\n",
    "Let $X = (x^{(1)}, x^{(2)}, \\dotsc, x^{(L)})$ be an input sequence of $L$ tokens, where each token, $x^{(i)} \\in \\mathbb{R}^d$, is represented as a $d$-dimensional learnable embedding; $X \\in \\mathbb{R}^{L \\times d}$. Let $W^K, W^Q, W^V \\in \\mathbb{R}^{d \\times d}$ be learnable projection matrices (unique per layer, head).\n",
    "\n",
    "Now, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "Q &= XW^Q = \\begin{bmatrix} \n",
    "\\text{--- } \\left(x^{(1)}\\right)^T W^Q \\text{ ---} \\\\\n",
    "\\text{--- } \\left(x^{(2)}\\right)^T W^Q \\text{ ---} \\\\\n",
    "\\vdots \\\\\n",
    "\\text{--- } \\left(x^{(L)}\\right)^T W^Q \\text{ ---}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{L \\times d} \\\\\n",
    "    K &= XW^K \\in \\mathbb{R}^{L \\times d} \\\\\n",
    "    V &= XW^V \\in \\mathbb{R}^{L \\times d}\n",
    "\\end{align}$$\n",
    "\n",
    "Still thinking one head at a time, our idea was basically create the following structure:\n",
    "\n",
    "Let us apply a (trainable) filter kernel to mimic $QK^T \\in \\mathbb{R}^{L \\times L}$ (viewed as $\\mathbb{R}^{L^2}$). Unlike project 2, when we worked with circulant matrices, the expected structure of the Attention matrix (as shown in the images before) would be associated with a Toeplitz matrix, with a kernell ($\\in \\mathbb{R}^{2L-1}$) such as the image below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1oNeIa1hcPhorHrRa8Y4y3vCWUErVL9pw\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which generates now a circulant structure such as (we remark the dark boudries around the piece equivalent to the \"original\" attention ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1LpwEbBCN5VT50POw6P_JJrron24SCv44\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix we call the Toeplitz Augmentation Unit (_TAU_), where the dimensions are increased to accommodate the kernel and create the easy and fast to multiply circulant matrix. This happens because the convolution operation can be thought of as the linear mapping ${H = Z^\\ast \\Sigma Z}$ (where $Z$ is the fast-Fourier matrix):\n",
    "$$\\begin{align}\n",
    "    \\tau &= Z^\\ast \\Sigma Z\n",
    "\\end{align}$$\n",
    "\n",
    "This $\\tau$ \"matrix\" that accounts for the \"weights\" has then to multiply $X$ to generate the new embeddings, and later on multiplied by another matrix that accounts for reprojection and across dimension (along $d$) interaction. Hence this part's entire operation is:\n",
    "\n",
    "$$Z^\\ast \\Sigma ZXW^\\tau$$\n",
    "\n",
    "The other part, the sparse matrix doesn't have such refinement as our $\\tau$ unit, but demands some explanation. From a mathematical perspective, to force a sparse matrix we used two techniques:\n",
    "\n",
    "1. **Thresholding**: We forced our sparse matrix to have at most 30\\% of its entries non zero. If during backpropagation it happens to have more then 30\\% of the entries above zero we then keep only the 30\\% larger entries and zero out all the other ones.\n",
    "2. **Regularization**: Aiming for some control over the sparse matrix, and avoid values that are too large, we also added to our loss function a penalization for the value in the entries of the sparse matrix, in other words, we added the norm of the matrix to the loss function.\n",
    "\n",
    "Unfortunately, for NLP reasons (expressiveness and reprojections) we still have to multiply $S$ by a parameter matrix $W^S, which reduces a little bit the speed up we were aiming for, but still resulted in a faster approach and with promissing accuracy results. In summary, what we did was:\n",
    "\n",
    "$$softmax\\left(\\frac{QK^T}{\\sqrt{d}}V\\right) \\approx (S (X W^X) W^S + Z^\\ast \\Sigma Z(XW^X)W^\\tau)$$\n",
    "\n",
    "Given that the original paper that first computed the Attention Matrix does not use parallelism we also decided to not use it, and compare both structures with regular sequencial scripts, but we would like to remark that we could still deliver better results by making some further adaptations:\n",
    "\n",
    "a. Creating faster sparse structures and operations wraping around Pytorch current classes\n",
    "b. Implementing parallelism in the multiple parts of our approach where it would be possible\n",
    "\n",
    "To better illustrate the mentioned performance improvement we wrote testing scripts to evaluate execution time of the Attention Module and our Togepi Module to different sized random inputs. The results are in the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_PS: You are more than welcome to go over the code, but it is not necessary for the understanding of the results, hence you could jump directly to the end of the notebook._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X16P1qt0DwBQ"
   },
   "source": [
    "### togepi\n",
    "\n",
    "toeplitz-based generative pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "odGTA2H8ooLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Cphj1grouWQ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-3etjA5-6CRJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "import math\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OaWkttpbalof"
   },
   "outputs": [],
   "source": [
    "def print_params(module, print_vals=True):\n",
    "    params_table = PrettyTable(['module', 'num_params', 'requires_grad'])\n",
    "    total_trainable_params = 0\n",
    "    for name, param in module.named_parameters():\n",
    "        params_table.add_row([name, param.numel(), param.requires_grad])\n",
    "        if param.requires_grad:\n",
    "            total_trainable_params = total_trainable_params + param.numel()\n",
    "    print(params_table)\n",
    "    if total_trainable_params > 1e6:\n",
    "        print(f'total trainable params: {(total_trainable_params / 1e6):0.2f}M')\n",
    "    else:\n",
    "        print(f'total trainable params: {total_trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0Fo5xwdELFD"
   },
   "source": [
    "#### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZkXaehG6ayY",
    "outputId": "ec9f6745-30f8-4756-e23a-347d21c417ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class TestTogepiConfig:\n",
    "    # embedding\n",
    "    vocab_size = 10  # includes special tokens ([PAD], [MASK], [CLS], [SEP]) \n",
    "    padding_idx = 0\n",
    "    max_position_embeddings = 7  # includes proxy for padding token; max_length = max_position_embeddings - 1\n",
    "    pad_position = 0\n",
    "    num_token_types = 3  # includes padding token type\n",
    "    pad_token_type = 0\n",
    "    embedding_dim = 4\n",
    "    embedding_dropout_proba = 0.1\n",
    "    \n",
    "    # attention\n",
    "    causal_attn = True  # for generative pre-training\n",
    "    num_attn_heads = 2\n",
    "    attn_actn = 'gelu'\n",
    "    sparse_dens = 0.3\n",
    "    attn_dropout_proba = 0.1\n",
    "\n",
    "test_config = TestTogepiConfig()\n",
    "test_config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuaE3p2WENJY"
   },
   "source": [
    "#### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IS2PvH-2SY5",
    "outputId": "8bc6244f-8125-4d57-8b99-1a7b178718ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|   tok_emb.weight  |     40     |      True     |\n",
      "|   pos_emb.weight  |     28     |      True     |\n",
      "|  type_emb.weight  |     12     |      True     |\n",
      "| layer_norm.weight |     4      |      True     |\n",
      "|  layer_norm.bias  |     4      |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.7468,  1.0223, -0.1759,  0.9004],\n",
       "          [ 0.7343,  1.2877, -0.0000, -1.6012],\n",
       "          [-1.7097,  1.3248, -0.1124,  0.4973],\n",
       "          [ 1.1672,  1.0530, -1.0768, -1.1434],\n",
       "          [ 0.8496,  0.2151, -1.8746,  0.8099],\n",
       "          [ 0.8496,  0.2151, -1.8746,  0.8099]],\n",
       " \n",
       "         [[-1.5472,  1.3251, -0.5070,  0.7291],\n",
       "          [ 0.5841,  1.3160, -0.2221, -1.6780],\n",
       "          [-1.1318, -0.9334,  1.6043,  0.4609],\n",
       "          [ 0.5997,  1.4710, -1.4034, -0.6673],\n",
       "          [-0.0000,  1.4685, -0.9210,  0.6706],\n",
       "          [-1.6175,  0.0000,  0.6469, -0.3610]]], grad_fn=<MulBackward0>),\n",
       " torch.Size([2, 6, 4]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self._padding_idx = config.padding_idx\n",
    "        self._pad_position = config.pad_position\n",
    "        self._pad_token_type = config.pad_token_type\n",
    "\n",
    "        self.tok_emb = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embedding_dim, padding_idx=config.padding_idx)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=config.max_position_embeddings, embedding_dim=config.embedding_dim, padding_idx=config.pad_position)\n",
    "        self.type_emb = nn.Embedding(num_embeddings=config.num_token_types, embedding_dim=config.embedding_dim, padding_idx=config.pad_token_type)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.tok_emb.weight.data)\n",
    "        self.tok_emb.weight.data[self._padding_idx] = torch.zeros(config.embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.pos_emb.weight.data)\n",
    "        self.tok_emb.weight.data[self._pad_position] = torch.zeros(config.embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.type_emb.weight.data)\n",
    "        self.tok_emb.weight.data[self._pad_token_type] = torch.zeros(config.embedding_dim)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.embedding_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=config.embedding_dropout_proba)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, padding_mask=None):\n",
    "        # input_ids: (batch_size, max_length)\n",
    "        # padding_mask: (batch_size, max_length)\n",
    "        max_length = input_ids.shape[1]\n",
    "        # assert(max_length == self.pos_emb.num_embeddings - 1)\n",
    "        if padding_mask is None:\n",
    "            # 1: no pad, 0: pad\n",
    "            padding_mask = torch.where(input_ids == self._padding_idx, 0, 1)\n",
    "\n",
    "        # position_ids: (batch_size, max_length)\n",
    "        # assert(self._pad_position == 0)\n",
    "        position_ids = torch.arange(max_length, dtype=torch.long, device=input_ids.device) + 1  # assuming zero is reserved for pad position\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_ids = position_ids.masked_fill(padding_mask == 0, self._pad_position)\n",
    "\n",
    "        # token_type_ids: (batch_size, max_length)\n",
    "        if token_type_ids is None:\n",
    "            # assert(self._pad_token_type == 0)\n",
    "            token_type_ids = torch.ones_like(input_ids)  # assuming zero is reserved for pad position\n",
    "        token_type_ids = token_type_ids.masked_fill(padding_mask == 0, self._pad_token_type)\n",
    "        \n",
    "        token_embeddings = self.tok_emb(input_ids)\n",
    "        position_embeddings = self.pos_emb(position_ids)\n",
    "        token_type_embeddings = self.type_emb(token_type_ids)\n",
    "        \n",
    "        return self.dropout(self.layer_norm(token_embeddings + position_embeddings + token_type_embeddings))\n",
    "\n",
    "test_input_ids = torch.tensor([[1, 2, 3, 4, 0, 0], [3, 4, 5, 6, 7, 8]])\n",
    "test_emb_obj = Embedding(test_config)\n",
    "test_emb = test_emb_obj(test_input_ids)\n",
    "print_params(test_emb_obj)\n",
    "test_emb, test_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAesn44GEPCh"
   },
   "source": [
    "#### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UpIZhrjhyoMo",
    "outputId": "f3003553-cd95-4e69-c89a-d0f783e0b188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|     wq.weight     |     16     |      True     |\n",
      "|      wq.bias      |     4      |      True     |\n",
      "|     wk.weight     |     16     |      True     |\n",
      "|      wk.bias      |     4      |      True     |\n",
      "|     wv.weight     |     16     |      True     |\n",
      "|      wv.bias      |     4      |      True     |\n",
      "|     wo.weight     |     16     |      True     |\n",
      "|      wo.bias      |     4      |      True     |\n",
      "| layer_norm.weight |     4      |      True     |\n",
      "|  layer_norm.bias  |     4      |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.2223,  0.5883, -0.6726,  1.3067],\n",
       "          [-0.2960,  1.3438,  0.3586, -1.4064],\n",
       "          [-1.4819, -0.0752,  0.2372,  1.3199],\n",
       "          [-0.1796,  1.5546, -0.1372, -1.2378],\n",
       "          [-1.0302,  0.7668, -0.9444,  1.2078],\n",
       "          [-1.0423,  0.9359, -0.9548,  1.0612]],\n",
       " \n",
       "         [[-1.1850, -0.0741, -0.3184,  1.5776],\n",
       "          [-0.9913,  1.0017,  0.9983, -1.0087],\n",
       "          [ 1.1699, -1.5675,  0.4169, -0.0194],\n",
       "          [-1.1786,  1.5858, -0.1229, -0.2843],\n",
       "          [-1.2998,  1.1983, -0.6086,  0.7100],\n",
       "          [ 0.0582, -1.6466,  0.9034,  0.6850]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([2, 6, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        assert(config.embedding_dim % config.num_attn_heads == 0)\n",
    "\n",
    "        self._num_heads = config.num_attn_heads\n",
    "        self._per_head_dim = config.embedding_dim // config.num_attn_heads\n",
    "        max_length = config.max_position_embeddings - 1\n",
    "\n",
    "        self.wq = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        self.wk = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        self.wv = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.wq.weight.data)\n",
    "        nn.init.xavier_normal_(self.wk.weight.data)\n",
    "        nn.init.xavier_normal_(self.wv.weight.data)\n",
    "\n",
    "        self._causal = config.causal_attn\n",
    "        if config.causal_attn:\n",
    "            self.register_buffer('causal_attn_mask', torch.tril(torch.ones(max_length, max_length)).view(1, 1, max_length, max_length))\n",
    "\n",
    "        self.wo = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.wo.weight.data)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.embedding_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=config.attn_dropout_proba)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def _extend_padding_mask(self, padding_mask, embeddings):\n",
    "        # padding_mask: (batch_size, max_length)\n",
    "        if padding_mask is None:\n",
    "            padding_mask = torch.ones(embeddings.shape[0], embeddings.shape[1])\n",
    "\n",
    "        extended_padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_padding_mask = extended_padding_mask.to(dtype=embeddings.dtype)  # amp/fp16 compatibility\n",
    "        extended_padding_mask = (1 - extended_padding_mask) * -1e4\n",
    "        return extended_padding_mask\n",
    "\n",
    "    def forward(self, embeddings, padding_mask=None):\n",
    "        batch_size = embeddings.shape[0]\n",
    "        max_length = embeddings.shape[1]\n",
    "        embedding_dim = embeddings.shape[2]\n",
    "\n",
    "        # embeddings: (batch_size, max_length, embedding_dim)\n",
    "        # attn_mask: 1 = non-pad, 0 = pad\n",
    "        # projected_*: (batch_size, max_length, num_heads * per_head_dim)\n",
    "        projected_query = self.wq(embeddings)\n",
    "        projected_key = self.wk(embeddings)\n",
    "        projected_value = self.wv(embeddings)\n",
    "\n",
    "        sliced_projected_query = projected_query.view(batch_size, max_length, self._num_heads, self._per_head_dim).permute(0, 2, 1, 3)\n",
    "        sliced_projected_key_tr = projected_query.view(batch_size, max_length, self._num_heads, self._per_head_dim).permute(0, 2, 3, 1)\n",
    "        sliced_projected_value = projected_query.view(batch_size, max_length, self._num_heads, self._per_head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # attn_mat: (batch_size, num_heads, max_length, max_length)\n",
    "        # attn_mat: QK' / sqrt(d)\n",
    "        # attn_mask: set [pad] tok attn values to -inf\n",
    "        attn_mat = torch.matmul(sliced_projected_query, sliced_projected_key_tr) / np.power(embedding_dim, 0.5)\n",
    "        attn_mat = attn_mat + self._extend_padding_mask(padding_mask=padding_mask, embeddings=embeddings)\n",
    "        if self._causal:\n",
    "            attn_mat.masked_fill_(self.causal_attn_mask[:, :, :max_length, :max_length] == 0, -1e4)\n",
    "        # attn_probs: (batch_size, num_heads, max_length, max_length)\n",
    "        attn_probs = self.softmax(attn_mat)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        # ctx_vectors: (batch_size, num_heads, max_length, per_head_dim)\n",
    "        #    .permute: (batch_size, max_length, num_heads, per_head_dim)\n",
    "        #    .view   : (batch_size, max_length, num_heads * per_head_dim)\n",
    "        ctx_vectors = torch.matmul(attn_probs, sliced_projected_value).permute(0, 2, 1, 3).contiguous().view(batch_size, max_length, -1)\n",
    "        attn_output = self.wo(ctx_vectors)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "\n",
    "        return self.layer_norm(attn_output + embeddings), attn_probs\n",
    "\n",
    "test_mha_obj = MultiHeadAttention(test_config)\n",
    "test_padding_mask = torch.tensor([[1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1]])\n",
    "test_mha_emb, test_mha_filters = test_mha_obj(test_emb, padding_mask=test_padding_mask)\n",
    "print_params(test_mha_obj)\n",
    "test_mha_emb, test_mha_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBsd2ezN6vFE",
    "outputId": "5033a646-93aa-46c2-8ef9-c3740ff57018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------+---------------+\n",
      "|         module         | num_params | requires_grad |\n",
      "+------------------------+------------+---------------+\n",
      "|     toeplitz_psfs      |     44     |      True     |\n",
      "|         sparse         |     36     |      True     |\n",
      "|    pre_proj.weight     |     16     |      True     |\n",
      "|     pre_proj.bias      |     4      |      True     |\n",
      "| pre_sparse_proj.weight |     16     |      True     |\n",
      "|  pre_sparse_proj.bias  |     4      |      True     |\n",
      "| post_conv_proj.weight  |     16     |      True     |\n",
      "|  post_conv_proj.bias   |     4      |      True     |\n",
      "|   layer_norm.weight    |     4      |      True     |\n",
      "|    layer_norm.bias     |     4      |      True     |\n",
      "+------------------------+------------+---------------+\n",
      "total trainable params: 148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.6801,  0.7827,  0.1640,  0.7334],\n",
       "          [ 0.3760,  1.4663, -0.8449, -0.9974],\n",
       "          [-1.5970,  1.1562,  0.1317,  0.3091],\n",
       "          [ 0.9828,  0.9007, -1.4156, -0.4679],\n",
       "          [ 0.7972,  0.1035, -1.6645,  0.7637],\n",
       "          [-0.1368,  0.4456, -1.5210,  1.2122]],\n",
       " \n",
       "         [[-1.4156,  1.1995, -0.4087,  0.6248],\n",
       "          [ 0.2022,  1.5489, -0.7595, -0.9916],\n",
       "          [-0.6960, -1.0655,  1.5246,  0.2369],\n",
       "          [ 0.3313,  1.1090, -1.6210,  0.1807],\n",
       "          [ 0.1517,  1.2346, -1.5568,  0.1705],\n",
       "          [-1.0827,  0.1097, -0.5957,  1.5687]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([2, 6, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TogepiMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        assert(config.embedding_dim % config.num_attn_heads == 0)\n",
    "\n",
    "        self._num_heads = config.num_attn_heads\n",
    "        self._per_head_dim = config.embedding_dim // config.num_attn_heads\n",
    "        max_length = config.max_position_embeddings - 1  # one position reserved for pad position\n",
    "        self._training_max_length = max_length\n",
    "\n",
    "        # out_features: (num_heads * per_head_dim)\n",
    "        self.pre_proj = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        self.pre_sparse_proj = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.pre_proj.weight.data)\n",
    "        nn.init.xavier_normal_(self.pre_sparse_proj.weight.data)\n",
    "\n",
    "        # randomly initialize point-spread functions, one per head\n",
    "        # psf: [tok_weight, [tok_-1_weights, tok_-2_weight, ...], [..., tok_+2_weight, tok_+1_weight]]\n",
    "        self.toeplitz_psfs = nn.Parameter(torch.randn(self._num_heads, 2 * max_length - 1, self._per_head_dim))\n",
    "        self.attn_actn = F.gelu if config.attn_actn == 'gelu' else F.relu\n",
    "        self.post_conv_proj = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.toeplitz_psfs.data)\n",
    "        nn.init.xavier_normal_(self.post_conv_proj.weight.data)\n",
    "        \n",
    "        num_nonzero = int(max_length * max_length * config.sparse_dens)\n",
    "        sparse_idxs = torch.randint(0, max_length, (num_nonzero, 2))\n",
    "        sparse_vals = torch.randn(num_nonzero)\n",
    "        self.sparse = nn.Parameter(torch.sparse_coo_tensor(sparse_idxs.t(), sparse_vals.abs(), size=(max_length, max_length)).to_dense())\n",
    "\n",
    "        self._causal = config.causal_attn\n",
    "        if config.causal_attn:\n",
    "            # causal_psf_mask: ignore the tokens appearing ahead of the current token.\n",
    "            self.register_buffer('causal_psf_mask', torch.tensor([1] + [1] * (max_length - 1) + [0] * (max_length - 1)).unsqueeze(0).unsqueeze(2))\n",
    "            self.register_buffer('causal_sparse_mask', torch.tril(torch.ones(max_length, max_length)))\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.embedding_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=config.attn_dropout_proba)\n",
    "\n",
    "    def forward(self, embeddings, padding_mask=None, softmax_psf_weights=True):\n",
    "        # embeddings: (batch_size, max_length, embedding_dim)\n",
    "        # padding_mask: (batch_size, max_length)\n",
    "        batch_size = embeddings.shape[0]\n",
    "        max_length = embeddings.shape[1]\n",
    "        embedding_dim = embeddings.shape[2]\n",
    "\n",
    "        # expanded_padding_mask: (batch_size, max_length, 1)\n",
    "        # 1: no pad, 0: pad\n",
    "        expanded_padding_mask = None\n",
    "        if padding_mask is not None:\n",
    "            expanded_padding_mask = padding_mask.unsqueeze(2)\n",
    "\n",
    "        # pre_proj_emb: (batch_size, max_length, num_heads * per_head_dim)\n",
    "        pre_proj_emb = self.pre_proj(embeddings)\n",
    "        if padding_mask is not None:\n",
    "            pre_proj_emb.masked_fill_(expanded_padding_mask == 0, 0)\n",
    "        # padded_embeddings: (batch_size, 2 * max_length - 1, embedding_dim)\n",
    "        # F.pad: pad=(padding_left, padding_right, padding_top, padding_bottom)\n",
    "        pre_proj_padded_embeddings = F.pad(pre_proj_emb, pad=(0, 0, 0, max_length - 1), mode='constant')\n",
    "        # pre_proj_padded_embeddings: (batch_size, num_heads, 2 * max_length - 1, per_head_dim)\n",
    "        pre_proj_padded_embeddings = pre_proj_padded_embeddings.view(batch_size, 2 * max_length - 1, self._num_heads, self._per_head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        psfs_weights = self.toeplitz_psfs.data\n",
    "        if self._causal:\n",
    "            if self._training_max_length == max_length:\n",
    "                psfs_weights.masked_fill_(self.causal_psf_mask == 0, 0)\n",
    "            else:\n",
    "                # at inference time, the max_length changes per prompt\n",
    "                causal_psf_mask = torch.tensor([1] + [1] * (max_length - 1) + [0] * (max_length - 1)).unsqueeze(0).unsqueeze(2)\n",
    "                psfs_weights.masked_fill_(causal_psf_mask == 0, 0)\n",
    "        if softmax_psf_weights:\n",
    "            psfs_weights = F.softmax(psfs_weights, dim=1)\n",
    "        psfs_fft = torch.fft.fftn(psfs_weights, dim=(1, 2))\n",
    "        emb_fft = torch.fft.fftn(pre_proj_padded_embeddings, dim=(2, 3))\n",
    "        # conv_output: (batch_size, num_heads, max_length, per_head_dim)\n",
    "        conv_output = torch.real(torch.fft.ifftn(psfs_fft * emb_fft, dim=(2, 3))[:, :, :max_length, :])\n",
    "        # conv_output: (batch_size, max_length, num_heads * per_head_dim)\n",
    "        conv_output = self.attn_actn(conv_output).permute(0, 2, 1, 3).reshape(batch_size, max_length, -1)\n",
    "        conv_emb = self.post_conv_proj(conv_output)\n",
    "        \n",
    "        \n",
    "        sparse_data = self.sparse.data\n",
    "        if self._causal:\n",
    "            sparse_data.masked_fill_(self.causal_sparse_mask[:max_length, :max_length] == 0, 0)\n",
    "        pre_sparse_emb = self.pre_sparse_proj(pre_proj_emb)\n",
    "        if padding_mask is not None:\n",
    "            pre_sparse_emb.masked_fill_(expanded_padding_mask == 0, 0)\n",
    "        sparse_emb = torch.matmul(sparse_data, pre_sparse_emb)\n",
    "\n",
    "        togepi_emb = self.dropout(conv_emb + sparse_emb)\n",
    "        return self.layer_norm(togepi_emb + embeddings)\n",
    "        \n",
    "test_togepi_mha_obj = TogepiMultiHeadAttention(test_config)\n",
    "test_padding_mask = torch.tensor([[1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1]])\n",
    "test_togepi_mha_emb = test_togepi_mha_obj(test_emb, padding_mask=test_padding_mask)\n",
    "print_params(test_togepi_mha_obj)\n",
    "test_togepi_mha_emb, test_togepi_mha_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0InxTFWwXucA"
   },
   "source": [
    "---\n",
    "#### speed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqdFx1HL5KQX"
   },
   "source": [
    "##### *sparse vs. dense*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Y1I-wlDzXu2U"
   },
   "outputs": [],
   "source": [
    "def create_sparse_mat(sparse_dens=0.3, max_length=512):\n",
    "    num_nonzero = int(max_length * max_length * sparse_dens)\n",
    "    sparse_idxs = torch.randint(0, max_length, (num_nonzero, 2))\n",
    "    sparse_vals = torch.randn(num_nonzero)\n",
    "    return torch.sparse_coo_tensor(sparse_idxs.t(), sparse_vals.abs(), size=(max_length, max_length))\n",
    "\n",
    "def create_emb(batch_size=32, max_length=512, embedding_dim=768):\n",
    "    return torch.randn(batch_size, max_length, embedding_dim)\n",
    "\n",
    "def sparse_matmul(sparse_mat, emb):\n",
    "    # sparse_mat: (max_length, max_length) \n",
    "    batch_size, max_length, embedding_dim = emb.shape\n",
    "    return torch.sparse.mm(sparse_mat, emb.permute(1, 0, 2).reshape(max_length, -1)).view(max_length, batch_size, -1).permute(1, 0, 2)\n",
    "\n",
    "def sparse_to_dense_matmul(sparse_mat, emb):\n",
    "    # sparse_mat: (max_length, max_length)\n",
    "    return torch.matmul(sparse_mat.to_dense(), emb)\n",
    "\n",
    "def dense_matmul(dense_mat, emb):\n",
    "    return torch.matmul(dense_mat, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "29QuyWMgbwOh"
   },
   "outputs": [],
   "source": [
    "sparse_dens = 0.3\n",
    "max_length = 512\n",
    "batch_size = 32 \n",
    "embedding_dim = 768\n",
    "\n",
    "sparse_mat = create_sparse_mat(sparse_dens=sparse_dens, max_length=max_length)\n",
    "dense_mat = sparse_mat.to_dense()\n",
    "emb = create_emb(batch_size=batch_size, max_length=max_length, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CNbIpmKb4Lp",
    "outputId": "479ad195-8b21-4ac5-c13f-cf569852fa9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 ms ± 2.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sparse_matmul(sparse_mat, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLiLG1SZjTar",
    "outputId": "e0ab9b88-de17-4b7c-bf17-a364517d6a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ms ± 199 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sparse_to_dense_matmul(sparse_mat, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEHNrjhpcL5W",
    "outputId": "996f3fc0-df61-4586-d739-fb539b191168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.9 ms ± 215 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dense_matmul(dense_mat, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IVC89fH5QWV"
   },
   "source": [
    "##### *bert-attention vs. togepi-attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWVPTpJEoyBA",
    "outputId": "066cf14a-6285-42e3-9960-d2194cb832c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://aclanthology.org/2021.emnlp-main.831.pdf\n",
    "@dataclass\n",
    "class SpeedTestConfig:\n",
    "    # embedding\n",
    "    vocab_size = 30522\n",
    "    padding_idx = 0\n",
    "    max_position_embeddings = 1024 + 1 #L\n",
    "    pad_position = 0\n",
    "    num_token_types = 3\n",
    "    pad_token_type = 0\n",
    "    embedding_dim = 2048 #d\n",
    "    embedding_dropout_proba = 0.1\n",
    "    \n",
    "    # attention\n",
    "    causal_attn = True  # for generative pre-training\n",
    "    num_attn_heads = 16\n",
    "    attn_actn = 'gelu'\n",
    "    sparse_dens = 0.3\n",
    "    attn_dropout_proba = 0.1\n",
    "\n",
    "    # training\n",
    "    batch_size = 64\n",
    "    \n",
    "test_speed_config = SpeedTestConfig()\n",
    "test_speed_config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cck1ynh8OGK",
    "outputId": "832aa860-6c0d-40a1-be48-5e678f1d3adc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_ids = torch.randint(low=0, high=test_speed_config.max_position_embeddings - 1, \n",
    "                               size=(test_speed_config.batch_size, test_speed_config.max_position_embeddings - 1))\n",
    "test_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jggMcqVj9ATi",
    "outputId": "a8f9f338-7eae-4e93-ef6f-b5fc3cf631c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|   tok_emb.weight  |  62509056  |      True     |\n",
      "|   pos_emb.weight  |  2099200   |      True     |\n",
      "|  type_emb.weight  |    6144    |      True     |\n",
      "| layer_norm.weight |    2048    |      True     |\n",
      "|  layer_norm.bias  |    2048    |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 64.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024, 2048])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb_obj = Embedding(test_speed_config)\n",
    "test_emb = test_emb_obj(test_input_ids)\n",
    "print_params(test_emb_obj)\n",
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evRhxzobnNGX",
    "outputId": "672afd27-c25c-49ec-f9b5-3b04cc020fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|     wq.weight     |  4194304   |      True     |\n",
      "|      wq.bias      |    2048    |      True     |\n",
      "|     wk.weight     |  4194304   |      True     |\n",
      "|      wk.bias      |    2048    |      True     |\n",
      "|     wv.weight     |  4194304   |      True     |\n",
      "|      wv.bias      |    2048    |      True     |\n",
      "|     wo.weight     |  4194304   |      True     |\n",
      "|      wo.bias      |    2048    |      True     |\n",
      "| layer_norm.weight |    2048    |      True     |\n",
      "|  layer_norm.bias  |    2048    |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 16.79M\n",
      "+------------------------+------------+---------------+\n",
      "|         module         | num_params | requires_grad |\n",
      "+------------------------+------------+---------------+\n",
      "|     toeplitz_psfs      |  4192256   |      True     |\n",
      "|         sparse         |  1048576   |      True     |\n",
      "|    pre_proj.weight     |  4194304   |      True     |\n",
      "|     pre_proj.bias      |    2048    |      True     |\n",
      "| pre_sparse_proj.weight |  4194304   |      True     |\n",
      "|  pre_sparse_proj.bias  |    2048    |      True     |\n",
      "| post_conv_proj.weight  |  4194304   |      True     |\n",
      "|  post_conv_proj.bias   |    2048    |      True     |\n",
      "|   layer_norm.weight    |    2048    |      True     |\n",
      "|    layer_norm.bias     |    2048    |      True     |\n",
      "+------------------------+------------+---------------+\n",
      "total trainable params: 17.83M\n"
     ]
    }
   ],
   "source": [
    "test_mha_obj = MultiHeadAttention(test_speed_config)\n",
    "print_params(test_mha_obj)\n",
    "\n",
    "test_togepi_mha_obj = TogepiMultiHeadAttention(test_speed_config)\n",
    "print_params(test_togepi_mha_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55Xxrveg9KFc",
    "outputId": "539987c7-404f-4a73-f350-09effc46a129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 14s ± 6.16 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_mha_emb, test_mha_filters = test_mha_obj(test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GRmWUXC9p51",
    "outputId": "181819fa-98af-4168-95f5-e77b1b16c1c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 s ± 419 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_togepi_mha_emb = test_togepi_mha_obj(test_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEibQq6KtMy8"
   },
   "source": [
    "#### Conclusion and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, altough the improvement have effects in the speed of the NLP application, for the project regarding CS5223 we decided to make something very specific to compare those models. This jupyter notebook contains the functions and classes definitions and also some very basic tests. (Despite being basic in terms of dimensionality, you are more than welcome to run then and have fun). Seeking for more robust and concret results we ran both methods, Attention and Togepi, in a server (same type of GPU), to compare their performance in multiple different scenarios.\n",
    "\n",
    "We see an improvement across multiple cases. For very small settings (small $L$ and small $d$) Togepi doesn't seems to be faster because of some overhead that is necessary and not related to the size of the inputs. However the NLP challange is dealing with larger inputs, settings in which Togepi has consistently shown do be faster than Attention.\n",
    "\n",
    "Analyzing the complexity of both methods, when we take dimensions to infinity, they have indeed the same theoretical lower bound $\\mathcal{O}(L^2d)$, however, if we considerer each part of the code, the Attention method is $\\mathcal{O}(Ld^2) + \\mathcal{O}(L^2d) + \\mathcal{O}(L^2) + \\mathcal{O}(L^2d)$, while in our case, given the speed up from FFT we are only bounded by the three matrix multiplication at the sparse side which, disconsidering sparsity effect would be $\\mathcal{O}(L^2d)$.\n",
    "\n",
    "We show below the multiple tests we ran on the server:\n",
    "\n",
    "\n",
    "1. **Avg. Run Time (10 iterations) for 2 heads**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1XPHAybVYXW0CYspSKRD4bSPHgZKlm1xr\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1FYgutIR_kEHFyx43N_QktprPJwmlvD9d\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Avg. Run Time (10 iterations) for 8 heads**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1evwQAs5B6TddyQBL7wnhINssCKA_y5yG\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1jmN2-yJFIdenLo900ZeWyYqEIRft2-os\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Avg. Run Time (10 iterations) for 16 heads**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=11uyYS1ZC1AyHckrtodk-5y177b8lVB4o\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=16NsBNrScj5AQjQ7bz6joYlLB17fEeaKl\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thanks for the great semester and have a great summer*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

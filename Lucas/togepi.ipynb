{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 5223 - Grad Project\n",
    "Tushaar Gangavarapu - TG352 and Lucas Molter - LM865\n",
    "\n",
    "Professor: David Bindel\n",
    "\n",
    "May 19th 2023\n",
    "\n",
    "Cornell University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "The backbone of recent (and important) NLP developments has been the Attention matrix, first mentioned at the 2017 paper \"Attention is All You Need\". To explain it very briefly, this approach has allowed NLP models represent words as a weighted average "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X16P1qt0DwBQ"
   },
   "source": [
    "### togepi\n",
    "\n",
    "toeplitz-based generative pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "odGTA2H8ooLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Cphj1grouWQ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-3etjA5-6CRJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "import math\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OaWkttpbalof"
   },
   "outputs": [],
   "source": [
    "def print_params(module, print_vals=True):\n",
    "    params_table = PrettyTable(['module', 'num_params', 'requires_grad'])\n",
    "    total_trainable_params = 0\n",
    "    for name, param in module.named_parameters():\n",
    "        params_table.add_row([name, param.numel(), param.requires_grad])\n",
    "        if param.requires_grad:\n",
    "            total_trainable_params = total_trainable_params + param.numel()\n",
    "    print(params_table)\n",
    "    if total_trainable_params > 1e6:\n",
    "        print(f'total trainable params: {(total_trainable_params / 1e6):0.2f}M')\n",
    "    else:\n",
    "        print(f'total trainable params: {total_trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0Fo5xwdELFD"
   },
   "source": [
    "#### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZkXaehG6ayY",
    "outputId": "ec9f6745-30f8-4756-e23a-347d21c417ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class TestTogepiConfig:\n",
    "    # embedding\n",
    "    vocab_size = 10  # includes special tokens ([PAD], [MASK], [CLS], [SEP]) \n",
    "    padding_idx = 0\n",
    "    max_position_embeddings = 7  # includes proxy for padding token; max_length = max_position_embeddings - 1\n",
    "    pad_position = 0\n",
    "    num_token_types = 3  # includes padding token type\n",
    "    pad_token_type = 0\n",
    "    embedding_dim = 4\n",
    "    embedding_dropout_proba = 0.1\n",
    "    \n",
    "    # attention\n",
    "    causal_attn = True  # for generative pre-training\n",
    "    num_attn_heads = 2\n",
    "    attn_actn = 'gelu'\n",
    "    sparse_dens = 0.3\n",
    "    attn_dropout_proba = 0.1\n",
    "\n",
    "test_config = TestTogepiConfig()\n",
    "test_config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuaE3p2WENJY"
   },
   "source": [
    "#### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IS2PvH-2SY5",
    "outputId": "8bc6244f-8125-4d57-8b99-1a7b178718ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|   tok_emb.weight  |     40     |      True     |\n",
      "|   pos_emb.weight  |     28     |      True     |\n",
      "|  type_emb.weight  |     12     |      True     |\n",
      "| layer_norm.weight |     4      |      True     |\n",
      "|  layer_norm.bias  |     4      |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.7468,  1.0223, -0.1759,  0.9004],\n",
       "          [ 0.7343,  1.2877, -0.0000, -1.6012],\n",
       "          [-1.7097,  1.3248, -0.1124,  0.4973],\n",
       "          [ 1.1672,  1.0530, -1.0768, -1.1434],\n",
       "          [ 0.8496,  0.2151, -1.8746,  0.8099],\n",
       "          [ 0.8496,  0.2151, -1.8746,  0.8099]],\n",
       " \n",
       "         [[-1.5472,  1.3251, -0.5070,  0.7291],\n",
       "          [ 0.5841,  1.3160, -0.2221, -1.6780],\n",
       "          [-1.1318, -0.9334,  1.6043,  0.4609],\n",
       "          [ 0.5997,  1.4710, -1.4034, -0.6673],\n",
       "          [-0.0000,  1.4685, -0.9210,  0.6706],\n",
       "          [-1.6175,  0.0000,  0.6469, -0.3610]]], grad_fn=<MulBackward0>),\n",
       " torch.Size([2, 6, 4]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self._padding_idx = config.padding_idx\n",
    "        self._pad_position = config.pad_position\n",
    "        self._pad_token_type = config.pad_token_type\n",
    "\n",
    "        self.tok_emb = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embedding_dim, padding_idx=config.padding_idx)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=config.max_position_embeddings, embedding_dim=config.embedding_dim, padding_idx=config.pad_position)\n",
    "        self.type_emb = nn.Embedding(num_embeddings=config.num_token_types, embedding_dim=config.embedding_dim, padding_idx=config.pad_token_type)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.tok_emb.weight.data)\n",
    "        self.tok_emb.weight.data[self._padding_idx] = torch.zeros(config.embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.pos_emb.weight.data)\n",
    "        self.tok_emb.weight.data[self._pad_position] = torch.zeros(config.embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.type_emb.weight.data)\n",
    "        self.tok_emb.weight.data[self._pad_token_type] = torch.zeros(config.embedding_dim)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.embedding_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=config.embedding_dropout_proba)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, padding_mask=None):\n",
    "        # input_ids: (batch_size, max_length)\n",
    "        # padding_mask: (batch_size, max_length)\n",
    "        max_length = input_ids.shape[1]\n",
    "        # assert(max_length == self.pos_emb.num_embeddings - 1)\n",
    "        if padding_mask is None:\n",
    "            # 1: no pad, 0: pad\n",
    "            padding_mask = torch.where(input_ids == self._padding_idx, 0, 1)\n",
    "\n",
    "        # position_ids: (batch_size, max_length)\n",
    "        # assert(self._pad_position == 0)\n",
    "        position_ids = torch.arange(max_length, dtype=torch.long, device=input_ids.device) + 1  # assuming zero is reserved for pad position\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_ids = position_ids.masked_fill(padding_mask == 0, self._pad_position)\n",
    "\n",
    "        # token_type_ids: (batch_size, max_length)\n",
    "        if token_type_ids is None:\n",
    "            # assert(self._pad_token_type == 0)\n",
    "            token_type_ids = torch.ones_like(input_ids)  # assuming zero is reserved for pad position\n",
    "        token_type_ids = token_type_ids.masked_fill(padding_mask == 0, self._pad_token_type)\n",
    "        \n",
    "        token_embeddings = self.tok_emb(input_ids)\n",
    "        position_embeddings = self.pos_emb(position_ids)\n",
    "        token_type_embeddings = self.type_emb(token_type_ids)\n",
    "        \n",
    "        return self.dropout(self.layer_norm(token_embeddings + position_embeddings + token_type_embeddings))\n",
    "\n",
    "test_input_ids = torch.tensor([[1, 2, 3, 4, 0, 0], [3, 4, 5, 6, 7, 8]])\n",
    "test_emb_obj = Embedding(test_config)\n",
    "test_emb = test_emb_obj(test_input_ids)\n",
    "print_params(test_emb_obj)\n",
    "test_emb, test_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAesn44GEPCh"
   },
   "source": [
    "#### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UpIZhrjhyoMo",
    "outputId": "f3003553-cd95-4e69-c89a-d0f783e0b188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|     wq.weight     |     16     |      True     |\n",
      "|      wq.bias      |     4      |      True     |\n",
      "|     wk.weight     |     16     |      True     |\n",
      "|      wk.bias      |     4      |      True     |\n",
      "|     wv.weight     |     16     |      True     |\n",
      "|      wv.bias      |     4      |      True     |\n",
      "|     wo.weight     |     16     |      True     |\n",
      "|      wo.bias      |     4      |      True     |\n",
      "| layer_norm.weight |     4      |      True     |\n",
      "|  layer_norm.bias  |     4      |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.2223,  0.5883, -0.6726,  1.3067],\n",
       "          [-0.2960,  1.3438,  0.3586, -1.4064],\n",
       "          [-1.4819, -0.0752,  0.2372,  1.3199],\n",
       "          [-0.1796,  1.5546, -0.1372, -1.2378],\n",
       "          [-1.0302,  0.7668, -0.9444,  1.2078],\n",
       "          [-1.0423,  0.9359, -0.9548,  1.0612]],\n",
       " \n",
       "         [[-1.1850, -0.0741, -0.3184,  1.5776],\n",
       "          [-0.9913,  1.0017,  0.9983, -1.0087],\n",
       "          [ 1.1699, -1.5675,  0.4169, -0.0194],\n",
       "          [-1.1786,  1.5858, -0.1229, -0.2843],\n",
       "          [-1.2998,  1.1983, -0.6086,  0.7100],\n",
       "          [ 0.0582, -1.6466,  0.9034,  0.6850]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([2, 6, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        assert(config.embedding_dim % config.num_attn_heads == 0)\n",
    "\n",
    "        self._num_heads = config.num_attn_heads\n",
    "        self._per_head_dim = config.embedding_dim // config.num_attn_heads\n",
    "        max_length = config.max_position_embeddings - 1\n",
    "\n",
    "        self.wq = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        self.wk = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        self.wv = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.wq.weight.data)\n",
    "        nn.init.xavier_normal_(self.wk.weight.data)\n",
    "        nn.init.xavier_normal_(self.wv.weight.data)\n",
    "\n",
    "        self._causal = config.causal_attn\n",
    "        if config.causal_attn:\n",
    "            self.register_buffer('causal_attn_mask', torch.tril(torch.ones(max_length, max_length)).view(1, 1, max_length, max_length))\n",
    "\n",
    "        self.wo = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.wo.weight.data)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.embedding_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=config.attn_dropout_proba)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def _extend_padding_mask(self, padding_mask, embeddings):\n",
    "        # padding_mask: (batch_size, max_length)\n",
    "        if padding_mask is None:\n",
    "            padding_mask = torch.ones(embeddings.shape[0], embeddings.shape[1])\n",
    "\n",
    "        extended_padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_padding_mask = extended_padding_mask.to(dtype=embeddings.dtype)  # amp/fp16 compatibility\n",
    "        extended_padding_mask = (1 - extended_padding_mask) * -1e4\n",
    "        return extended_padding_mask\n",
    "\n",
    "    def forward(self, embeddings, padding_mask=None):\n",
    "        batch_size = embeddings.shape[0]\n",
    "        max_length = embeddings.shape[1]\n",
    "        embedding_dim = embeddings.shape[2]\n",
    "\n",
    "        # embeddings: (batch_size, max_length, embedding_dim)\n",
    "        # attn_mask: 1 = non-pad, 0 = pad\n",
    "        # projected_*: (batch_size, max_length, num_heads * per_head_dim)\n",
    "        projected_query = self.wq(embeddings)\n",
    "        projected_key = self.wk(embeddings)\n",
    "        projected_value = self.wv(embeddings)\n",
    "\n",
    "        sliced_projected_query = projected_query.view(batch_size, max_length, self._num_heads, self._per_head_dim).permute(0, 2, 1, 3)\n",
    "        sliced_projected_key_tr = projected_query.view(batch_size, max_length, self._num_heads, self._per_head_dim).permute(0, 2, 3, 1)\n",
    "        sliced_projected_value = projected_query.view(batch_size, max_length, self._num_heads, self._per_head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # attn_mat: (batch_size, num_heads, max_length, max_length)\n",
    "        # attn_mat: QK' / sqrt(d)\n",
    "        # attn_mask: set [pad] tok attn values to -inf\n",
    "        attn_mat = torch.matmul(sliced_projected_query, sliced_projected_key_tr) / np.power(embedding_dim, 0.5)\n",
    "        attn_mat = attn_mat + self._extend_padding_mask(padding_mask=padding_mask, embeddings=embeddings)\n",
    "        if self._causal:\n",
    "            attn_mat.masked_fill_(self.causal_attn_mask[:, :, :max_length, :max_length] == 0, -1e4)\n",
    "        # attn_probs: (batch_size, num_heads, max_length, max_length)\n",
    "        attn_probs = self.softmax(attn_mat)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        # ctx_vectors: (batch_size, num_heads, max_length, per_head_dim)\n",
    "        #    .permute: (batch_size, max_length, num_heads, per_head_dim)\n",
    "        #    .view   : (batch_size, max_length, num_heads * per_head_dim)\n",
    "        ctx_vectors = torch.matmul(attn_probs, sliced_projected_value).permute(0, 2, 1, 3).contiguous().view(batch_size, max_length, -1)\n",
    "        attn_output = self.wo(ctx_vectors)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "\n",
    "        return self.layer_norm(attn_output + embeddings), attn_probs\n",
    "\n",
    "test_mha_obj = MultiHeadAttention(test_config)\n",
    "test_padding_mask = torch.tensor([[1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1]])\n",
    "test_mha_emb, test_mha_filters = test_mha_obj(test_emb, padding_mask=test_padding_mask)\n",
    "print_params(test_mha_obj)\n",
    "test_mha_emb, test_mha_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBsd2ezN6vFE",
    "outputId": "5033a646-93aa-46c2-8ef9-c3740ff57018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------+---------------+\n",
      "|         module         | num_params | requires_grad |\n",
      "+------------------------+------------+---------------+\n",
      "|     toeplitz_psfs      |     44     |      True     |\n",
      "|         sparse         |     36     |      True     |\n",
      "|    pre_proj.weight     |     16     |      True     |\n",
      "|     pre_proj.bias      |     4      |      True     |\n",
      "| pre_sparse_proj.weight |     16     |      True     |\n",
      "|  pre_sparse_proj.bias  |     4      |      True     |\n",
      "| post_conv_proj.weight  |     16     |      True     |\n",
      "|  post_conv_proj.bias   |     4      |      True     |\n",
      "|   layer_norm.weight    |     4      |      True     |\n",
      "|    layer_norm.bias     |     4      |      True     |\n",
      "+------------------------+------------+---------------+\n",
      "total trainable params: 148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.6801,  0.7827,  0.1640,  0.7334],\n",
       "          [ 0.3760,  1.4663, -0.8449, -0.9974],\n",
       "          [-1.5970,  1.1562,  0.1317,  0.3091],\n",
       "          [ 0.9828,  0.9007, -1.4156, -0.4679],\n",
       "          [ 0.7972,  0.1035, -1.6645,  0.7637],\n",
       "          [-0.1368,  0.4456, -1.5210,  1.2122]],\n",
       " \n",
       "         [[-1.4156,  1.1995, -0.4087,  0.6248],\n",
       "          [ 0.2022,  1.5489, -0.7595, -0.9916],\n",
       "          [-0.6960, -1.0655,  1.5246,  0.2369],\n",
       "          [ 0.3313,  1.1090, -1.6210,  0.1807],\n",
       "          [ 0.1517,  1.2346, -1.5568,  0.1705],\n",
       "          [-1.0827,  0.1097, -0.5957,  1.5687]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([2, 6, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TogepiMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        assert(config.embedding_dim % config.num_attn_heads == 0)\n",
    "\n",
    "        self._num_heads = config.num_attn_heads\n",
    "        self._per_head_dim = config.embedding_dim // config.num_attn_heads\n",
    "        max_length = config.max_position_embeddings - 1  # one position reserved for pad position\n",
    "        self._training_max_length = max_length\n",
    "\n",
    "        # out_features: (num_heads * per_head_dim)\n",
    "        self.pre_proj = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        self.pre_sparse_proj = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.pre_proj.weight.data)\n",
    "        nn.init.xavier_normal_(self.pre_sparse_proj.weight.data)\n",
    "\n",
    "        # randomly initialize point-spread functions, one per head\n",
    "        # psf: [tok_weight, [tok_-1_weights, tok_-2_weight, ...], [..., tok_+2_weight, tok_+1_weight]]\n",
    "        self.toeplitz_psfs = nn.Parameter(torch.randn(self._num_heads, 2 * max_length - 1, self._per_head_dim))\n",
    "        self.attn_actn = F.gelu if config.attn_actn == 'gelu' else F.relu\n",
    "        self.post_conv_proj = nn.Linear(in_features=config.embedding_dim, out_features=config.embedding_dim)\n",
    "        nn.init.xavier_normal_(self.toeplitz_psfs.data)\n",
    "        nn.init.xavier_normal_(self.post_conv_proj.weight.data)\n",
    "        \n",
    "        num_nonzero = int(max_length * max_length * config.sparse_dens)\n",
    "        sparse_idxs = torch.randint(0, max_length, (num_nonzero, 2))\n",
    "        sparse_vals = torch.randn(num_nonzero)\n",
    "        self.sparse = nn.Parameter(torch.sparse_coo_tensor(sparse_idxs.t(), sparse_vals.abs(), size=(max_length, max_length)).to_dense())\n",
    "\n",
    "        self._causal = config.causal_attn\n",
    "        if config.causal_attn:\n",
    "            # causal_psf_mask: ignore the tokens appearing ahead of the current token.\n",
    "            self.register_buffer('causal_psf_mask', torch.tensor([1] + [1] * (max_length - 1) + [0] * (max_length - 1)).unsqueeze(0).unsqueeze(2))\n",
    "            self.register_buffer('causal_sparse_mask', torch.tril(torch.ones(max_length, max_length)))\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.embedding_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=config.attn_dropout_proba)\n",
    "\n",
    "    def forward(self, embeddings, padding_mask=None, softmax_psf_weights=True):\n",
    "        # embeddings: (batch_size, max_length, embedding_dim)\n",
    "        # padding_mask: (batch_size, max_length)\n",
    "        batch_size = embeddings.shape[0]\n",
    "        max_length = embeddings.shape[1]\n",
    "        embedding_dim = embeddings.shape[2]\n",
    "\n",
    "        # expanded_padding_mask: (batch_size, max_length, 1)\n",
    "        # 1: no pad, 0: pad\n",
    "        expanded_padding_mask = None\n",
    "        if padding_mask is not None:\n",
    "            expanded_padding_mask = padding_mask.unsqueeze(2)\n",
    "\n",
    "        # pre_proj_emb: (batch_size, max_length, num_heads * per_head_dim)\n",
    "        pre_proj_emb = self.pre_proj(embeddings)\n",
    "        if padding_mask is not None:\n",
    "            pre_proj_emb.masked_fill_(expanded_padding_mask == 0, 0)\n",
    "        # padded_embeddings: (batch_size, 2 * max_length - 1, embedding_dim)\n",
    "        # F.pad: pad=(padding_left, padding_right, padding_top, padding_bottom)\n",
    "        pre_proj_padded_embeddings = F.pad(pre_proj_emb, pad=(0, 0, 0, max_length - 1), mode='constant')\n",
    "        # pre_proj_padded_embeddings: (batch_size, num_heads, 2 * max_length - 1, per_head_dim)\n",
    "        pre_proj_padded_embeddings = pre_proj_padded_embeddings.view(batch_size, 2 * max_length - 1, self._num_heads, self._per_head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        psfs_weights = self.toeplitz_psfs.data\n",
    "        if self._causal:\n",
    "            if self._training_max_length == max_length:\n",
    "                psfs_weights.masked_fill_(self.causal_psf_mask == 0, 0)\n",
    "            else:\n",
    "                # at inference time, the max_length changes per prompt\n",
    "                causal_psf_mask = torch.tensor([1] + [1] * (max_length - 1) + [0] * (max_length - 1)).unsqueeze(0).unsqueeze(2)\n",
    "                psfs_weights.masked_fill_(causal_psf_mask == 0, 0)\n",
    "        if softmax_psf_weights:\n",
    "            psfs_weights = F.softmax(psfs_weights, dim=1)\n",
    "        psfs_fft = torch.fft.fftn(psfs_weights, dim=(1, 2))\n",
    "        emb_fft = torch.fft.fftn(pre_proj_padded_embeddings, dim=(2, 3))\n",
    "        # conv_output: (batch_size, num_heads, max_length, per_head_dim)\n",
    "        conv_output = torch.real(torch.fft.ifftn(psfs_fft * emb_fft, dim=(2, 3))[:, :, :max_length, :])\n",
    "        # conv_output: (batch_size, max_length, num_heads * per_head_dim)\n",
    "        conv_output = self.attn_actn(conv_output).permute(0, 2, 1, 3).reshape(batch_size, max_length, -1)\n",
    "        conv_emb = self.post_conv_proj(conv_output)\n",
    "        \n",
    "        \n",
    "        sparse_data = self.sparse.data\n",
    "        if self._causal:\n",
    "            sparse_data.masked_fill_(self.causal_sparse_mask[:max_length, :max_length] == 0, 0)\n",
    "        pre_sparse_emb = self.pre_sparse_proj(pre_proj_emb)\n",
    "        if padding_mask is not None:\n",
    "            pre_sparse_emb.masked_fill_(expanded_padding_mask == 0, 0)\n",
    "        sparse_emb = torch.matmul(sparse_data, pre_sparse_emb)\n",
    "\n",
    "        togepi_emb = self.dropout(conv_emb + sparse_emb)\n",
    "        return self.layer_norm(togepi_emb + embeddings)\n",
    "        \n",
    "test_togepi_mha_obj = TogepiMultiHeadAttention(test_config)\n",
    "test_padding_mask = torch.tensor([[1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1]])\n",
    "test_togepi_mha_emb = test_togepi_mha_obj(test_emb, padding_mask=test_padding_mask)\n",
    "print_params(test_togepi_mha_obj)\n",
    "test_togepi_mha_emb, test_togepi_mha_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0InxTFWwXucA"
   },
   "source": [
    "---\n",
    "#### speed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqdFx1HL5KQX"
   },
   "source": [
    "##### *sparse vs. dense*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Y1I-wlDzXu2U"
   },
   "outputs": [],
   "source": [
    "def create_sparse_mat(sparse_dens=0.3, max_length=512):\n",
    "    num_nonzero = int(max_length * max_length * sparse_dens)\n",
    "    sparse_idxs = torch.randint(0, max_length, (num_nonzero, 2))\n",
    "    sparse_vals = torch.randn(num_nonzero)\n",
    "    return torch.sparse_coo_tensor(sparse_idxs.t(), sparse_vals.abs(), size=(max_length, max_length))\n",
    "\n",
    "def create_emb(batch_size=32, max_length=512, embedding_dim=768):\n",
    "    return torch.randn(batch_size, max_length, embedding_dim)\n",
    "\n",
    "def sparse_matmul(sparse_mat, emb):\n",
    "    # sparse_mat: (max_length, max_length) \n",
    "    batch_size, max_length, embedding_dim = emb.shape\n",
    "    return torch.sparse.mm(sparse_mat, emb.permute(1, 0, 2).reshape(max_length, -1)).view(max_length, batch_size, -1).permute(1, 0, 2)\n",
    "\n",
    "def sparse_to_dense_matmul(sparse_mat, emb):\n",
    "    # sparse_mat: (max_length, max_length)\n",
    "    return torch.matmul(sparse_mat.to_dense(), emb)\n",
    "\n",
    "def dense_matmul(dense_mat, emb):\n",
    "    return torch.matmul(dense_mat, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "29QuyWMgbwOh"
   },
   "outputs": [],
   "source": [
    "sparse_dens = 0.3\n",
    "max_length = 512\n",
    "batch_size = 32 \n",
    "embedding_dim = 768\n",
    "\n",
    "sparse_mat = create_sparse_mat(sparse_dens=sparse_dens, max_length=max_length)\n",
    "dense_mat = sparse_mat.to_dense()\n",
    "emb = create_emb(batch_size=batch_size, max_length=max_length, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CNbIpmKb4Lp",
    "outputId": "479ad195-8b21-4ac5-c13f-cf569852fa9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 ms ± 2.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sparse_matmul(sparse_mat, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLiLG1SZjTar",
    "outputId": "e0ab9b88-de17-4b7c-bf17-a364517d6a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ms ± 199 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sparse_to_dense_matmul(sparse_mat, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEHNrjhpcL5W",
    "outputId": "996f3fc0-df61-4586-d739-fb539b191168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.9 ms ± 215 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dense_matmul(dense_mat, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IVC89fH5QWV"
   },
   "source": [
    "##### *bert-attention vs. togepi-attention*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWVPTpJEoyBA",
    "outputId": "066cf14a-6285-42e3-9960-d2194cb832c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://aclanthology.org/2021.emnlp-main.831.pdf\n",
    "@dataclass\n",
    "class SpeedTestConfig:\n",
    "    # embedding\n",
    "    vocab_size = 30522\n",
    "    padding_idx = 0\n",
    "    max_position_embeddings = 1024 + 1 #L\n",
    "    pad_position = 0\n",
    "    num_token_types = 3\n",
    "    pad_token_type = 0\n",
    "    embedding_dim = 2048 #d\n",
    "    embedding_dropout_proba = 0.1\n",
    "    \n",
    "    # attention\n",
    "    causal_attn = True  # for generative pre-training\n",
    "    num_attn_heads = 16\n",
    "    attn_actn = 'gelu'\n",
    "    sparse_dens = 0.3\n",
    "    attn_dropout_proba = 0.1\n",
    "\n",
    "    # training\n",
    "    batch_size = 64\n",
    "    \n",
    "test_speed_config = SpeedTestConfig()\n",
    "test_speed_config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cck1ynh8OGK",
    "outputId": "832aa860-6c0d-40a1-be48-5e678f1d3adc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_ids = torch.randint(low=0, high=test_speed_config.max_position_embeddings - 1, \n",
    "                               size=(test_speed_config.batch_size, test_speed_config.max_position_embeddings - 1))\n",
    "test_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jggMcqVj9ATi",
    "outputId": "a8f9f338-7eae-4e93-ef6f-b5fc3cf631c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|   tok_emb.weight  |  62509056  |      True     |\n",
      "|   pos_emb.weight  |  2099200   |      True     |\n",
      "|  type_emb.weight  |    6144    |      True     |\n",
      "| layer_norm.weight |    2048    |      True     |\n",
      "|  layer_norm.bias  |    2048    |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 64.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024, 2048])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb_obj = Embedding(test_speed_config)\n",
    "test_emb = test_emb_obj(test_input_ids)\n",
    "print_params(test_emb_obj)\n",
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evRhxzobnNGX",
    "outputId": "672afd27-c25c-49ec-f9b5-3b04cc020fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------------+\n",
      "|       module      | num_params | requires_grad |\n",
      "+-------------------+------------+---------------+\n",
      "|     wq.weight     |  4194304   |      True     |\n",
      "|      wq.bias      |    2048    |      True     |\n",
      "|     wk.weight     |  4194304   |      True     |\n",
      "|      wk.bias      |    2048    |      True     |\n",
      "|     wv.weight     |  4194304   |      True     |\n",
      "|      wv.bias      |    2048    |      True     |\n",
      "|     wo.weight     |  4194304   |      True     |\n",
      "|      wo.bias      |    2048    |      True     |\n",
      "| layer_norm.weight |    2048    |      True     |\n",
      "|  layer_norm.bias  |    2048    |      True     |\n",
      "+-------------------+------------+---------------+\n",
      "total trainable params: 16.79M\n",
      "+------------------------+------------+---------------+\n",
      "|         module         | num_params | requires_grad |\n",
      "+------------------------+------------+---------------+\n",
      "|     toeplitz_psfs      |  4192256   |      True     |\n",
      "|         sparse         |  1048576   |      True     |\n",
      "|    pre_proj.weight     |  4194304   |      True     |\n",
      "|     pre_proj.bias      |    2048    |      True     |\n",
      "| pre_sparse_proj.weight |  4194304   |      True     |\n",
      "|  pre_sparse_proj.bias  |    2048    |      True     |\n",
      "| post_conv_proj.weight  |  4194304   |      True     |\n",
      "|  post_conv_proj.bias   |    2048    |      True     |\n",
      "|   layer_norm.weight    |    2048    |      True     |\n",
      "|    layer_norm.bias     |    2048    |      True     |\n",
      "+------------------------+------------+---------------+\n",
      "total trainable params: 17.83M\n"
     ]
    }
   ],
   "source": [
    "test_mha_obj = MultiHeadAttention(test_speed_config)\n",
    "print_params(test_mha_obj)\n",
    "\n",
    "test_togepi_mha_obj = TogepiMultiHeadAttention(test_speed_config)\n",
    "print_params(test_togepi_mha_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55Xxrveg9KFc",
    "outputId": "539987c7-404f-4a73-f350-09effc46a129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 14s ± 6.16 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_mha_emb, test_mha_filters = test_mha_obj(test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GRmWUXC9p51",
    "outputId": "181819fa-98af-4168-95f5-e77b1b16c1c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 s ± 419 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_togepi_mha_emb = test_togepi_mha_obj(test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2zopOptsLlE"
   },
   "outputs": [],
   "source": [
    "# https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html\n",
    "!pip install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wekn1XYb93OH",
    "outputId": "d869ea5e-9caa-4d21-873c-0fce3377fd04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3453.45 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit\n",
    "test_mha_emb, test_mha_filters = test_mha_obj(test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlaY--4UsGC_",
    "outputId": "d8dc5ce7-6e4e-4829-9c5e-262f3e8ce7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4036.32 MiB, increment: 1178.54 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "test_togepi_mha_emb = test_togepi_mha_obj(test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEibQq6KtMy8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
